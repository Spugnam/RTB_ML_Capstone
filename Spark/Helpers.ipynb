{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import isnan, when, count, col, split, udf, lower\nfrom pyspark.sql.functions import ltrim, rtrim\nfrom pyspark.sql.types import IntegerType, StringType\nfrom urlparse import urlparse"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["def create_dataframe_with_filter(files_path, column, value_filter_list):\n  \"\"\"Create subset dataframe with filter on column 'column' where value = 'value_filter_list'\n  \"\"\"\n  # create list of filenames\n  filenames = []\n  for file in dbutils.fs.ls(files_path):\n    if file[0].find('part-')> -1:\n      filenames.append(file[0])\n  print \"Number of partition files: \", len(filenames)\n  print \"\\n\".join(filenames)  # file list \"pretty\" print\n  # loop through partition files to create new dataframe\n  df = spark.read.parquet(filenames[0]).where(col(column).isin(value_filter_list)).cache() \n  print(\"First partition size: \", df.count())\n  for file in filenames[1:]:  # iterate from 2nd file\n    print(\"Adding \", file, \"...\")\n    df_temp = spark.read.parquet(file).where(col(column).isin(value_filter_list))\n    print(\"Number of rows added: \", df_temp.count())\n    df = df.union(df_temp).cache()  # believe it's better to cache since will reuse\n  print(\"Final dataframe size: \", df.count())"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Impressions/ Clicks merge function\n\ndef merge_imp_clicks(impressions, clicks):\n  clicks = clicks.select('impId').dropDuplicates(subset = ['impId']).withColumnRenamed('impId','clicked')\n  return(impressions.dropDuplicates(subset = [\"auctionId\"]).join(clicks,\n                          [impressions.auctionId == clicks.clicked], 'left').withColumn('clicked', clicks.clicked.isNotNull().cast('double')))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Impressions/ Clicks merge function\nfrom pyspark.sql.functions import broadcast\n\ndef merge_imp_clicks2(impressions, clicks):\n  clicks = clicks.select('impId').dropDuplicates(subset = ['impId']).withColumnRenamed('impId','clicked')\n  return(impressions.dropDuplicates(subset = [\"auctionId\"]).join(broadcast(clicks),\n                          [impressions.auctionId == clicks.clicked], 'left').withColumn('clicked', clicks.clicked.isNotNull().cast('double')))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#Chung Meng\n# Print Table of Column Missingness\ndef view_column_missingness(df):\n  display(df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]))\n\n# Print Structure of Data Frame with Custom Message \ndef print_struct_df (df,msg='Data Frame Structure'):\n  print(msg)\n  print(df.count(),len(df.columns))\n  print(df.columns)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["#Chung Meng\n# adSize => adWidth, adHeight\n# adArea = adWidth * adHeight \ndef cleanup_adSize(df):\n  split_col = split(df['adSize'], 'x')\n  df = df.withColumn('adWidth', split_col.getItem(0).cast(IntegerType()))\n  df = df.withColumn('adHeight', split_col.getItem(1).cast(IntegerType()))\n  df = df.withColumn('adArea',df.adWidth*df.adHeight)\n  df = df.drop('adHeight','adWidth')\n  return(df)\n  #df.show(5)\n  #df.printSchema()\n  "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Chung Meng\n# ageGroup as Categorical Feature \ndef ageToGroup(num):\n    if num is None:\n      return('none')    #Age is NA : None\n    elif num > 55:\n        return \"senior\" #Age > 55 : Senior\n    elif num > 35:\n        return \"middle\" # 55 > Age > 35 : Middle \n    elif num > 20:\n        return \"young\"  # 35 > Age > 20 : Young\n    elif num > 11:\n        return \"teen\"   # 20 > Age > 11 : Teen\n    elif num > 0:\n        return \"kid\"    # Age < 11 : Kid\n    else:\n        return \"none\"\n      \n      \ndef cleanup_age_category(df):\n  # Create a Column expression representing a UDF.\n  udfAgeGroup=udf(ageToGroup, StringType())\n\n  # apply UDF to age column\n  df=df.withColumn(\"ageGroup\", udfAgeGroup(\"age\"))\n  df=df.drop('age')\n# df.show(10)\n  return(df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#Chung Meng\ndef cleanup_age_numeric(df):\n  df=df.na.fill({'age': -999})\n  return(df.withColumn(\"age\", df['age'].astype(\"int\")))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["def cleanup_gender(df):\n  df=df.na.fill({'gender': u'U'})\n  return(df)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Chung Meng: OS Cleanup \n#def cleanup_os(df):\n#  df=df.na.fill({'os': u'unknown'})\n#  return(df)\n\n# ageGroup as Categorical Feature \ndef osToGroup(ostype):\n    if ostype is None:\n      return('unknown')\n    elif (ostype=='android')|(ostype=='ios'): #android/os unchanged\n      return(ostype)\n    elif ostype.find('window'): #windows & windows phone os => windows\n      return('windows')\n    else:\n      return(\"others\")  #the rest => others\n      \ndef cleanup_os(df):\n  # Create a Column expression representing a UDF.\n  udfOSGroup=udf(osToGroup, StringType())\n\n  # apply UDF to age column\n  df=df.withColumn(\"os\", udfOSGroup(\"os\"))\n  return(df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql.functions import concat\nfrom pyspark.sql.types import StringType\n\ndef create_interaction(col1, col2):\n  return(concat(col1, lit(\"-\"), col2))\n      \nudfinteraction = udf(create_interaction, StringType())\n\ndef interaction(df, col1, col2, new_col):\n  # create interation\n  return(df.withColumn(new_col, create_interaction(col1, col2)))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["def dedupe(df, subset):\n  return df.dropDuplicates(subset)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["def plot_evaluation_curve(models, labels, data, curve_type='roc', styles = ['b--','g--','r--','c--','m--','y--','k--','w--']):\n  '''\n  use to plot either receiver operating characteristic (roc) or precision-recall (pr) curve\n  models: list of models\n  labels: names of the models - used by plot legend\n  '''\n  import matplotlib.pyplot as plt\n  fig, ax = plt.subplots()\n  ax.set_xlim((0.0, 1.0))\n  ax.set_ylim((0.0, 1.0))\n\n  for i in range(len(models)):\n    model_fe = models[i].copy()\n    model_fe.stages = model_fe.stages[:-1]\n    modelSummary = models[i].stages[-1].evaluate(model_fe.transform(data))\n\n    if curve_type=='roc':\n      ax.plot(modelSummary.roc.toPandas().iloc[:,0], modelSummary.roc.toPandas().iloc[:,1], styles[i], label=labels[i])\n      ax.set_xlabel('FPR')    \n      ax.set_ylabel('TPR')   \n    else:\n      ax.plot(modelSummary.pr.toPandas().iloc[:,0], modelSummary.pr.toPandas().iloc[:,1], styles[i], label=labels[i])\n      ax.set_xlabel('Recall')\n      ax.set_ylabel('Precision')\n  ax.legend(loc='best', shadow=True)\n  display(fig)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["def take_a_sample(root_path = '/mnt/sito/capstone/2017-11/september/', child_path = './impressions/', start_row = 3, sample_ratio = 0.1):\n  '''\n  cycle through and take random sample from each parquet\n  '''\n  import os\n  file_infos = dbutils.fs.ls(os.path.join(root_path, child_path))[start_row:]\n  s = None\n  for file_info in file_infos:\n    if s is None:\n      s = spark.read.parquet(file_info.path).sample(False, sample_ratio, seed=0)\n    else:\n      s = s.union(spark.read.parquet(file_info.path).sample(False, sample_ratio, seed=0))\n  return s"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["def cleanup_country(df):\n  df = df.filter(lower(df.country) == 'usa')\n  return df.drop('country')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["def cleanup_timestamp(df):\n  from pyspark.sql.types import IntegerType\n  from pyspark.sql.functions import hour\n  weekday = udf(lambda dt: dt.weekday(), IntegerType())\n  df = df.withColumn('timestamp', df.timestamp.cast('timestamp'))\n  df = df.withColumn('timestamp_hour', hour(df.timestamp).cast('string')) \\\n         .withColumn('timestamp_weekday', weekday(df.timestamp).cast('string')) \\\n         .drop('timestamp')\n  return df"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\nfrom pyspark.sql.functions import udf, col, lit\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark import keyword_only  # This fails: from pyspark.ml.util import keyword_only. See https://github.com/maxpumperla/elephas/issues/60\n\nclass iabCategoriesTransformer(Transformer, HasInputCol, HasOutputCol):\n\n    @keyword_only\n    def __init__(self, inputCol=None, outputCol=None):\n      super(iabCategoriesTransformer, self).__init__() \n      kwargs = self._input_kwargs  # kwargs = self.__init__._input_kwargs\n      self.setParams(**kwargs)\n\n    @keyword_only\n    def setParams(self, inputCol=None, outputCol=None):\n        kwargs = self._input_kwargs  # kwargs = self.setParams._input_kwargs\n        return self._set(**kwargs)\n\n    def _transform(self, dataset):\n      out_col = [\"IAB\"+str(i) for i in range (1, 27)]  # self.getOutputCol()\n      in_col = dataset[self.getInputCol()]\n            \n      # Create udf functions\n      def read_cat(newcolName, iabCategoriescol):\n        \"\"\" Function to indicate if the column value is in the list of categories in original 'iabCategories' column\n        \"\"\"\n        try:\n          return(int(newcolName in iabCategoriescol))\n        except:\n          return(0)  # catch argument of type 'NoneType' is not iterable\n\n      def remove_subcat(iabCategoriescol):\n        \"\"\" Function to remove sub-categories from original 'iabCategories' column\n        \"\"\"\n        try:\n          return(list(map(lambda x: x.split(\"-\")[0], iabCategoriescol)))\n        except:\n          return(None)  # catch error argument 2 to map() must support iteration\n      \n      udf_read_cat = udf(read_cat, IntegerType())\n      udf_remove_subcat = udf(remove_subcat, ArrayType(StringType()))  \n      \n      # remove sub-categories from iabCategories column\n      # ._jc.toString().encode('utf8') to get column name\n      in_col_name = in_col._jc.toString().encode('utf8')\n      dataset = dataset.withColumn(in_col_name, udf_remove_subcat(in_col_name)) \n      for i in range(1, 27):\n        # create 26 'IAB' columns with their name in all values (to use in udf)\n        dataset = dataset.withColumn(out_col[i-1], lit(out_col[i-1]))\n        # use new column value to match in original column array of categories\n        dataset = dataset.withColumn(out_col[i-1], udf_read_cat(out_col[i-1], in_col_name)) \n        # return dataframe without original column\n      return(dataset.drop(in_col_name))\n    "],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql.functions import udf, col, lit\nfrom pyspark.sql.types import ArrayType, StringType, IntegerType\n\ndef read_cat(newcolName, iabCategoriescol):\n        \"\"\" Function to indicate if the column value is in the list of categories in original 'iabCategories' column\n        \"\"\"\n        try:\n          return(int(newcolName in iabCategoriescol))\n        except:\n          return(0)  # catch argument of type 'NoneType' is not iterable\n\ndef remove_subcat(iabCategoriescol):\n  \"\"\" Function to remove sub-categories from original 'iabCategories' column\n  \"\"\"\n  try:\n    return(list(map(lambda x: x.split(\"-\")[0], iabCategoriescol)))\n  except:\n    return(None)  # catch error argument 2 to map() must support iteration\n\nudf_read_cat = udf(read_cat, IntegerType())\nudf_remove_subcat = udf(remove_subcat, ArrayType(StringType()))  \n      \n  \ndef iab_encoder(dataset, in_col):\n  out_col = [\"IAB\"+str(i) for i in range (1, 27)]\n  # remove sub-categories from iabCategories column\n  # ._jc.toString().encode('utf8') to get column name\n  dataset = dataset.withColumn(in_col, udf_remove_subcat(in_col)) \n  for i in range(1, 27):\n    # create 26 'IAB' columns with their name in all values (to use in udf)\n    dataset = dataset.withColumn(out_col[i-1], lit(out_col[i-1]))\n    # use new column value to match in original column array of categories\n    dataset = dataset.withColumn(out_col[i-1], udf_read_cat(out_col[i-1], in_col)) \n    # return dataframe without original column\n  return(dataset.drop(in_col))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["def bidReq_select(df):\n  \n  return df.select(\n    df[\"timestamp\"].alias(\"bid_tstamp\"),\n    df[\"bidRequest\"][\"id\"].alias(\"bid_id\"),\n    df[\"bidRequest\"][\"app\"][\"bundle\"].alias(\"bid_app_bundle\"), \n    df[\"bidRequest\"][\"app\"][\"domain\"].alias(\"bid_app_domain\"), \n    df[\"bidRequest\"][\"app\"][\"publisher\"][\"name\"].alias(\"bid_app_publisher_name\"), \n    df[\"bidRequest\"][\"bcat\"].alias(\"bid_bcat\"),  \n    df[\"bidRequest\"][\"device\"][\"geo\"][\"city\"].alias(\"bid_city\"), \n    df[\"bidRequest\"][\"device\"][\"geo\"][\"metro\"].alias(\"bid_metro_goog\"), \n    df[\"bidRequest\"][\"device\"][\"geo\"][\"type\"].alias(\"bid_geo_type\"),\n    df[\"bidRequest\"][\"device\"][\"h\"].alias(\"bid_device_height\"), \n    df[\"bidRequest\"][\"device\"][\"w\"].alias(\"bid_device_width\"), \n    df[\"bidRequest\"][\"device\"][\"lmt\"].alias(\"bid_device_lmt\"), \n    df[\"bidRequest\"][\"device\"][\"make\"].alias(\"bid_device_make\"), \n    df[\"bidRequest\"][\"device\"][\"model\"].alias(\"bid_device_model\"), \n    df[\"bidRequest\"][\"device\"][\"osv\"].alias(\"bid_device_osv\"),\n    df[\"bidRequest\"][\"imp\"][0][\"secure\"].alias(\"bid_imp_secure\"),\n    df[\"bidRequest\"][\"imp\"][0][\"instl\"].alias(\"bid_imp_interstitial\"),\n    df[\"bidRequest\"][\"imp\"][0][\"bidfloor\"].alias(\"bid_bidfloor\"),\n    df[\"bidRequest\"][\"imp\"][0][\"video\"][\"pos\"].alias(\"bid_vid_pos\"),\n    df[\"bidRequest\"][\"imp\"][0][\"banner\"][\"pos\"].alias(\"bid_ban_pos\")\n  )\n\ndef merge_imps_and_bids(imps, bids):\n  bids = bidReq_select(bids).withColumnRenamed('timestamp','bid_tstamp')\n  return imps.join(bids, imps[\"auctionId\"] == bids[\"bid_id\"], 'left') #drop duplicates after. Suggested subset = [\"auctionId\"]"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["def column_lower_strip(df,colname):\n  return(df.withColumn(colname, lower(col(colname))).withColumn(colname, ltrim(col(colname))).withColumn(colname, rtrim(col(colname))))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql.functions import when\n\ndef format_region(impressions):\n  impressions=column_lower_strip(impressions,'region')\n  return (impressions.withColumn(\n    'region',\n    when(impressions['region'] == '', \"blank_\")\n    .when(impressions['region']=='pennsylvania', 'pa')\n    .when(impressions['region']=='illinois', 'il')\n    .when(impressions['region']=='new york', 'ny')\n    .when(impressions['region']=='iowa', 'ia')\n    .when(impressions['region']=='nebraska', 'ne')\n    .when(impressions['region']=='washington', 'wa')\n    .when(impressions['region']=='new jersey', 'nj')\n    .when(impressions['region']=='california', 'ca')\n    .when(impressions['region']=='delaware', 'de')\n    .when(impressions['region']=='georgia', 'ga')\n    .when(impressions['region']=='ohio', 'oh')\n    .when(impressions['region']=='texas', 'tx')\n    .when(impressions['region']=='florida', 'fl')\n    .when(impressions['region']=='idaho', 'id')\n    .when(impressions['region']=='wyoming', 'wy')\n    .when(impressions['region']=='maryland', 'md')\n    .when(impressions['region']=='nevada', 'nv')\n    .when(impressions['region']=='alaska', 'ak')\n    .when(impressions['region']=='kentucky', 'ky')\n    .when(impressions['region']=='minnesota', 'mn')\n    .when(impressions['region']=='michigan', 'mi')\n    .when(impressions['region']=='north carolina', 'nc')\n    .when(impressions['region']=='virginia', 'va')\n    .when(impressions['region']=='indiana', 'in')\n    .when(impressions['region']=='arizona', 'az')\n    .when(impressions['region']=='maine', 'me')\n    .when(impressions['region']=='montana', 'mt')\n    .when(impressions['region']=='louisiana', 'la')\n    .when(impressions['region']=='massachusetts', 'ma')\n    .when(impressions['region']=='tennessee', 'tn')\n    .when(impressions['region']=='alabama', 'al')\n    .when(impressions['region']=='arkansas', 'ar')\n    .when(impressions['region']=='new hampshire', 'nh')\n    .when(impressions['region']=='new mexico', 'nm')\n    .when(impressions['region']=='wisconsin', 'wi')\n    .when(impressions['region']=='connecticut', 'ct')\n    .when(impressions['region']=='missouri', 'mo')\n    .when(impressions['region']=='mississippi', 'ms')\n    .when(impressions['region']=='colorado', 'co')\n    .when(impressions['region']=='district of columbia', 'dc')\n    .when(impressions['region']=='hawaii', 'hi')\n    .when(impressions['region']=='kansas', 'ks')\n    .when(impressions['region']=='oklahoma', 'ok')\n    .otherwise(impressions['region'])\n  ).na.fill('-999', subset = ['region']) \n         )  #.groupby('region').count().orderBy('count', ascending=False) #for count by region"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["import pyspark.sql.functions as func\ndef reduce_cardinality(df, colname, limit, mode=0):\n  \"\"\"Reduces cardinality for reduce_cardinality(df, cols, mode, limit)\n  Minority Groups will be coalesced to 'other' Group\n  df = dataframe, \n  cols = column name string or list of strings\n  When mode==0: \n    limit= Total Number of Distinct Groups\n  When mode==1:\n    limit = Minimum Frequency/Count to remain as Original Group\n  sub_groups = number of subgroups\n  \"\"\"\n  #colname='region'\n  groups=df.groupBy(colname).agg(func.count(colname).alias('Count')).sort('Count')\n  if(mode==0):\n    minor_groups=groups.limit(groups.count()-limit)\n  else:\n    minor_groups=groups.filter(groups.Count<limit)\n    \n  minor_list=minor_groups.select(col(colname).alias(\"name\")).collect()\n  \n  def coalesce_residuals(grpname):\n    for i in range(len(minor_list)):\n      if grpname == minor_list[i].name:\n        return('others')\n    return(grpname)\n  udfGroup = udf(coalesce_residuals, StringType())\n  #bc = pricerite_locations\n  df=df.withColumn(colname, udfGroup(colname))\n  return(df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["def clean_app(x):\n  stopwords = ['ios','android','320','50','250','a','the',\"a\", \"an\", \"as\", \"i\", \"we\", \"app\"]\n  try:\n    return([word for word in x\\\n            .replace('-', ' ').replace('_', ' ').replace('x', ' ').split()\\\n            if ((word.lower() not in stopwords) & (len(word) > 2))][0]) # return first non-stop word\n  except:\n    return(x)\n\ndef clean_bestVenueName(df): # TODO match with app name from apps\n  udfclean_bestVenueName = udf(clean_app, StringType())\n  return(df.withColumn('bestVenueName', udfclean_bestVenueName(df.bestVenueName)))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["def clean_landingPage(df):\n  getdomain = udf(lambda r: urlparse(r).netloc.replace('www.',''), StringType())\n  return(df.withColumn('landingPage', getdomain(df.landingPage)))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.mllib.clustering import KMeans\nfrom pyspark.sql.types import *\nimport pandas as pd\nimport numpy as np\nimport random\ndef add_kmcluster(impressions):\n  impressions=impressions.withColumn('lat',col('location')[0]).withColumn('lon',col('location')[1])\n  location_pd=impressions.select('auctionId','dimensions','lat','lon').toPandas()\n  #Random Geo Imputation within US\n  US_lat_north=46\n  US_lat_south=35\n  US_lon_west=-116\n  US_lon_east=-93\n  NA_lat=location_pd.lat.isnull()\n  NA_lon=location_pd.lon.isnull()\n  location_pd.loc[NA_lat,'lat'] = location_pd.loc[NA_lat,'lat'].apply(lambda v: random.choice(np.arange(US_lat_south,US_lat_north,0.05)))\n  location_pd.loc[NA_lon,'lon'] = location_pd.loc[NA_lon,'lon'].apply(lambda v: random.choice(np.arange(US_lon_west,US_lon_east,0.05)))\n  #Save Coordinates as Matrix\n  coords = location_pd.as_matrix(columns=['lat', 'lon'])\n  #Run KMeans\n  coords_rdd=sc.parallelize(coords)\n  model = KMeans.train(coords_rdd, 500, maxIterations=40, initializationMode=\"random\", seed=50, initializationSteps=5, epsilon=1e-4)\n  #Cluster Prediction & Labels \n  cluster_labels=model.predict(coords_rdd).collect()\n  num_clusters=model.k\n  message = 'Clustered %d points down to %d clusters, for %f compression'\n  print(message%(len(coords), num_clusters, 100*(1 - float(num_clusters) / len(location_pd))))\n  #Append Clusters to Location DFrame\n  location_pd['cluster']=cluster_labels\n  pdSchema = StructType([StructField(\"auctionId2\", StringType(), True),\n                         StructField(\"dimensions2\", StringType(), True),\n                         StructField(\"lat2\", DoubleType(), True),\n                           StructField(\"lon2\", DoubleType(), True),\n                           StructField(\"cluster\", StringType(), True)]) \n  print(pdSchema)\n  location_cluster=spark.createDataFrame(location_pd,pdSchema)\n  #Join Location & Impressions DFrame\n  impressions=impressions.join(location_cluster,\n                                   (impressions.lat==location_cluster.lat2)&(impressions.lon==location_cluster.lon2)&\n                                   (impressions.auctionId==location_cluster.auctionId2)&(impressions.dimensions==location_cluster.dimensions2),\"inner\").dropDuplicates()\n  return(impressions.drop('lat2','lon2','dimensions2','auctionId2'))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["import math\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import FloatType\n\nlogloss = udf(lambda l,p:-l*math.log(float(p[1]))-(1-l)*math.log(float(p[0])),FloatType())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["#functions that return features and feature names lists\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexerModel\n#TODO: import stringindexer and vectorassembler\ndef getFeatureCols(pipelinemodel):\n  for stage in pipelinemodel.stages:\n    if type(stage) == VectorAssembler:\n      return stage.getInputCols()\n  return ValueError(\"no vector assembler\")\n\ndef getFeatureNames(pipelinemodel):\n  '''\n  returns a generator with a list for each feature name ordered by the index (most common to least per the StringIndexer transformation)\n  '''\n  \n  for stage in pipelinemodel.stages:\n    if type(stage) == StringIndexerModel:\n      yield stage.labels\n  pass"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["print(\"Helper functions loaded\")"],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"Helpers","notebookId":867512899846959},"nbformat":4,"nbformat_minor":0}
